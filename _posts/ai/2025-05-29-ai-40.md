---
layout: post
title: AI系列40:qnn
date: 2025-05-29 04:30:00 +0800
categories: [AI系列]
tags: [AI系列]
---
QNN是指Qualcomm Neural Network，即高通神经网络，是高通推出的用于AI推理的软件开发套件（SDK），也称为Qualcomm AI Engine Direct. 

它允许开发者将各种AI框架模型转换并优化为在Qualcomm芯片上运行

专为Snapdragon平台优化

QNN的主要功能和特点：

模型转换和优化：:QNN提供工具来将PyTorch, TensorFlow, Keras, ONNX等流行框架模型转换到Qualcomm平台的模型格式.﻿

推理引擎：:QNN提供推理引擎，可以在Qualcomm芯片上运行转换后的模型，并进行推理.﻿

工具和API：:QNN提供各种工具和API来帮助开发者进行模型转换，优化，测试和调试.﻿

## ONNX
ONNX（Open Neural Network Exchange，开放神经网络交换）是一种用于表示机器学习模型的开放标准格式，旨在解决不同AI框架间的模型兼容性问题，实现跨平台、跨框架的模型共享与部署。

ONNX 提供统一的模型存储格式，允许开发者在不同框架（如 PyTorch、TensorFlow、MXNet）间无缝转换模型

例如，PyTorch 训练的模型可转为 ONNX 格式，在 TensorFlow 上部署

TensorFlow 与 ONNX 是 互补协作关系：TensorFlow 作为训练框架生成模型，ONNX 作为中间格式实现跨平台部署。

graph LR
A[TensorFlow 训练模型] --> B[tf2onnx 转换工具]
B --> C[ONNX 格式模型]
C --> D[ONNX Runtime/TensorRT/OpenVINO 等推理引擎]
D --> E[CPU/GPU/NPU 硬件部署]
## AI框架模型
AI框架模型（Framework-Specific Model）指依赖特定深度学习框架（如PyTorch、TensorFlow）构建、训练和保存的机器学习模型。

它包含完整的模型结构定义、权重参数及训练配置，但通常与原生框架深度绑定，难以直接跨平台部署。

框架模型是训练阶段的产物，与框架生态强耦合（如PyTorch模型需torch库加载

以PyTorch模型部署到高通芯片为例：

graph TB
A[PyTorch定义模型] --> B[GPU训练]
B --> C[保存为model.pt]
C --> D[导出ONNX格式]
D --> E[用QNN工具链量化编译]
E --> F[部署到Snapdragon NPU]

关键转换步骤：

框架模型（.pt）→ 中间格式（.onnx）→ 硬件优化格式（QNN二进制）

