---
layout: post
title: AI系列71:token是啥
date: 2025-06-26 05:30:00 +0800
categories: [AI系列]
tags: [AI系列]
---
Tokens（标记）是大语言模型（如 GPT、LLaMA、DeepSeek 等）处理文本的基本单位，它决定了模型如何“理解”和“生成”语言。以下是关于 tokens 的深度解析：

---

### 一、**Tokens 的本质**
1. **定义**  
   Token 是模型对文本进行**分段处理的最小单元**，可以是：
   - 一个单词（如 `apple`）
   - 一个子词（如 `un` + `forget` + `able` → `unforgettable`）
   - 一个字符（如中文的 `你`）
   - 标点符号（如 `!`）

2. **分词原理（Tokenization）**  
   通过**分词器（Tokenizer）** 将文本拆解为 tokens，核心算法包括：
   - **Byte Pair Encoding (BPE)**：GPT 系列、LLaMA 使用
   - **WordPiece**：BERT 系列使用
   - **SentencePiece**：跨语言模型通用

---

### 二、**Tokens 如何影响模型工作**
#### 1. **输入限制**
   - 模型有 **上下文窗口上限**（如 GPT-4 Turbo 支持 **128K tokens**），超出部分会被截断。
   - 示例：  
     `你好！` → 中文字符通常 **1字≈2 tokens**（实际：`你`=1 token, `好`=1 token, `！`=1 token → 总计 **3 tokens**）。

#### 2. **计算与成本**
   - **API 收费按 token 计费**（如 GPT-4 输入 $10/1M tokens，输出 $30/1M tokens）。
   - 模型推理时 **计算量正比于 token 数量**（显存占用、生成时间均受影响）。

#### 3. **生成控制**
   - `max_tokens` 参数限制生成长度（如设为 100 则输出不超过 100 tokens）。
   - 长文本生成需多次分段处理（受限于上下文窗口）。

---

### 三、**中英文 Token 差异对比**
| **文本类型** | **示例句子**          | **Token 数量** | **分词逻辑**                  |
|--------------|-----------------------|----------------|-------------------------------|
| 英文         | `Hello, world!`       | 3 tokens       | `["Hello", ",", " world!"]`   |
| 中文         | `你好，世界！`        | 6 tokens       | `["你", "好", "，", "世", "界", "！"]` |
| 混合文本     | `Python很棒！`        | 5 tokens       | `["Python", "很", "棒", "！"]` |

> 💡 **关键结论**：  
> - 英文 **1 token ≈ 0.75 个单词**  
> - 中文 **1 token ≈ 0.5~1 个汉字**（因分词器而异）  
> - **代码、公式** 消耗 tokens 较多（特殊符号密集）

---

### 四、**如何计算 Token 数量？**
#### 常用工具：
1. **OpenAI Tokenizer**（[在线工具](https://platform.openai.com/tokenizer)）  
   ![OpenAI Tokenizer 示例](https://example.com/tokenizer-demo.png)  
   *输入文本实时显示 token 分割结果*

2. **Hugging Face `transformers` 库**  
   ```python
   from transformers import GPT2Tokenizer
   tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
   text = "深度学习改变世界！"
   tokens = tokenizer.tokenize(text)  # ['深', '度', '学', '习', '改', '变', '世', '界', '！']
   print(len(tokens))  # 输出：9
   ```

3. **Tiktoken（OpenAI 官方库）**  
   ```python
   import tiktoken
   enc = tiktoken.encoding_for_model("gpt-4")
   print(enc.encode("你好！"))  # [33450, 362, 123] → 3 tokens
   ```

---

### 五、**Token 优化的实用技巧**
1. **精简提示词（Prompt）**  
   - 删除冗余描述（如将 “请用专业且详细的语气回答” → “专业回答：”）
2. **结构化输入**  
   - 用 JSON 代替自然语言描述参数  
   - 示例：  
     ❌ 低效：`用户叫张三，年龄30岁，查询北京天气`（12 tokens）  
     ✅ 高效：`{"name":"张三","age":30,"query":"北京天气"}`（8 tokens）
3. **压缩长文本**  
   - 使用 `gpt-3.5-turbo` 生成摘要再输入主模型
4. **调整生成参数**  
   - 设置 `max_tokens=合理值` 避免无效长输出

---

### 六、**高级知识：Token 与模型架构的关系**
- **位置编码（Positional Encoding）**：每个 token 的位置信息嵌入向量，使模型理解词序。
- **注意力机制（Attention）**：计算 token 之间的关联权重（计算复杂度随 token 数量平方增长）。
- **KV Cache**：推理时缓存历史 token 的 Key/Value 张量，减少重复计算。

> ⚠️ 当输入超过 **上下文窗口** 时，模型可能：
> - 丢失早期信息（如 GPT 的滑动窗口）
> - 直接报错（如 Claude 的硬截断）

---

### 💎 总结
Tokens 是 LLM 的“语言血液”，直接影响：
- **模型理解能力**（分词质量决定语义还原度）  
- **系统性能边界**（上下文长度限制）  
- **应用成本**（按 token 计费）  
掌握 token 的运作机制，是构建高效、低成本 AI 应用的基础技能。


1. 用 [OpenAI Tokenizer](https://platform.openai.com/tokenizer) 测试你的常用提示词消耗  
2. 在代码中集成 `tiktoken` 监控生产环境 token 用量  
3. 长文本场景优先选择支持 **128K+ tokens** 的模型（如 GPT-4 Turbo、Claude 3、DeepSeek-R1）